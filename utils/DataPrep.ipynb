{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DataPrep.ipynb","provenance":[],"collapsed_sections":["8-t6SaxzUN1X"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","source":["## Libraries"],"metadata":{"id":"8-t6SaxzUN1X"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re # to clean text\n","from nltk import word_tokenize"],"metadata":{"id":"VDsDQ6O5UNvJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Clean sentences\n","reference: https://ourcodingclub.github.io/tutorials/topic-modelling-python/"],"metadata":{"id":"s9tC3uQbUNAD"}},{"cell_type":"code","metadata":{"id":"0jmwWKxRUXkI"},"source":["def remove_links(tweet):\n","    '''Takes a string and removes web links from it'''\n","    tweet = re.sub(r'http\\S+', '', tweet) # remove http links\n","    tweet = tweet.strip('[link]') # remove [links]\n","    return tweet\n","\n","def remove_users(tweet):\n","    '''Takes a string and removes retweet and @user information'''\n","    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) # remove retweet\n","    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) # remove tweeted at\n","    return tweet"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# cleaning master function\n","def clean_tweet(tweet,my_punctuation,my_stopwords,word_rooter, bigrams=False, root=False):\n","    tweet = tweet.encode(\"ascii\", \"ignore\") # remove non-ASCII characters\n","    tweet = tweet.decode() # encode the string\n","    tweet = remove_users(tweet)\n","    tweet = remove_links(tweet)\n","    tweet = tweet.lower() # lower case\n","    tweet = re.sub('['+my_punctuation + ']+', ' ', tweet) # strip punctuation\n","    tweet = re.sub('\\s+', ' ', tweet) #remove double spacing\n","    tweet = re.sub('([0-9]+)', '', tweet) # remove numbers\n","    tweet_token_list = [word for word in tweet.split(' ')\n","                            if word not in my_stopwords] # remove stopwords\n","\n","    if root:\n","      tweet_token_list = [word_rooter(word) if '#' not in word else word\n","                        for word in tweet_token_list] # apply word rooter\n","    if bigrams:\n","        tweet_token_list = tweet_token_list+[tweet_token_list[i]+'_'+tweet_token_list[i+1]\n","                                            for i in range(len(tweet_token_list)-1)]\n","    tweet = ' '.join(tweet_token_list)\n","    return tweet"],"metadata":{"id":"E6-0aDDSUrio"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Frequencies"],"metadata":{"id":"2ZvSTssP1alM"}},{"cell_type":"code","source":["# reference: Natural Language Processing with Classification and Vector Spaces by DeepLearning.AI\n","def build_freqs(tweets, ys):\n","    \"\"\"Build frequencies.\n","    Input:\n","        tweets: a list of tweets\n","        ys: an m x 1 array with the sentiment label of each tweet\n","            (either 0 or 1)\n","    Output:\n","        freqs: a dictionary mapping each (word, sentiment) pair to its\n","        frequency\n","    \"\"\"\n","    # Convert np array to list since zip needs an iterable.\n","    # The squeeze is necessary or the list ends up with one element.\n","    # Also note that this is just a NOP if ys is already a list.\n","    yslist = np.squeeze(ys).tolist()\n","\n","    # Start with an empty dictionary and populate it by looping over all tweets\n","    # and over all processed words in each tweet.\n","    freqs = {}\n","    for y, tweet in zip(yslist, tweets):\n","      if not pd.isnull(tweet):\n","        for word in word_tokenize(tweet):\n","            pair = (word, y)\n","            if pair in freqs:\n","                freqs[pair] += 1\n","            else:\n","                freqs[pair] = 1    \n","    return freqs"],"metadata":{"id":"ZLfVCelR1df4"},"execution_count":null,"outputs":[]}]}